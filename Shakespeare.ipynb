{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.utils import get_file\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "path = get_file('shakespeare.txt', 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 고유 문자 집합 생성\n",
        "chars = sorted(set(text))\n",
        "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# 텍스트를 숫자로 변환\n",
        "max_length = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - max_length, step):\n",
        "    sentences.append(text[i: i + max_length])\n",
        "    next_chars.append(text[i + max_length])\n",
        "x = np.zeros((len(sentences), max_length, len(chars)), dtype=np.bool_)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_to_index[char]] = 1\n",
        "    y[i, char_to_index[next_chars[i]]] = 1\n",
        "\n",
        "# 모델 구축\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(max_length, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(x, y, batch_size=128, epochs=100)\n",
        "\n",
        "# 텍스트 생성 함수\n",
        "def generate_text(model, length, diversity):\n",
        "    start_index = np.random.randint(0, len(text) - max_length - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + max_length]\n",
        "    generated += sentence\n",
        "    for i in range(length):\n",
        "        x_pred = np.zeros((1, max_length, len(chars)), dtype=np.bool_)\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_to_index[char]] = 1.\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = np.random.choice(len(chars), p=preds)\n",
        "        next_char = index_to_char[next_index]\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return generated\n",
        "\n",
        "\n",
        "# 생성된 텍스트 출력\n",
        "print(generate_text(model, length=400, diversity=1.0))"
      ],
      "metadata": {
        "id": "tEpmG1VxwRYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c43f0e-d069-4b10-e722-4f85a83ac5db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n",
            "Epoch 1/100\n",
            "2905/2905 [==============================] - 18s 5ms/step - loss: 2.4169\n",
            "Epoch 2/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 2.0168\n",
            "Epoch 3/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 1.8696\n",
            "Epoch 4/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.7760\n",
            "Epoch 5/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.7110\n",
            "Epoch 6/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 1.6624\n",
            "Epoch 7/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.6228\n",
            "Epoch 8/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.5907\n",
            "Epoch 9/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.5640\n",
            "Epoch 10/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 1.5415\n",
            "Epoch 11/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.5220\n",
            "Epoch 12/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.5048\n",
            "Epoch 13/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.4893\n",
            "Epoch 14/100\n",
            "2905/2905 [==============================] - 17s 6ms/step - loss: 1.4758\n",
            "Epoch 15/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.4635\n",
            "Epoch 16/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.4519\n",
            "Epoch 17/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.4415\n",
            "Epoch 18/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.4320\n",
            "Epoch 19/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.4227\n",
            "Epoch 20/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.4140\n",
            "Epoch 21/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.4059\n",
            "Epoch 22/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.3985\n",
            "Epoch 23/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3918\n",
            "Epoch 24/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3855\n",
            "Epoch 25/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3790\n",
            "Epoch 26/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3732\n",
            "Epoch 27/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3673\n",
            "Epoch 28/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3617\n",
            "Epoch 29/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.3570\n",
            "Epoch 30/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3520\n",
            "Epoch 31/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3469\n",
            "Epoch 32/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3428\n",
            "Epoch 33/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3387\n",
            "Epoch 34/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3344\n",
            "Epoch 35/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3309\n",
            "Epoch 36/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3261\n",
            "Epoch 37/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3240\n",
            "Epoch 38/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3194\n",
            "Epoch 39/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3167\n",
            "Epoch 40/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3138\n",
            "Epoch 41/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3100\n",
            "Epoch 42/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.3071\n",
            "Epoch 43/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3036\n",
            "Epoch 44/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.3010\n",
            "Epoch 45/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2978\n",
            "Epoch 46/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2960\n",
            "Epoch 47/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2933\n",
            "Epoch 48/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2905\n",
            "Epoch 49/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 1.2881\n",
            "Epoch 50/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2857\n",
            "Epoch 51/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2837\n",
            "Epoch 52/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2812\n",
            "Epoch 53/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2793\n",
            "Epoch 54/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2767\n",
            "Epoch 55/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2751\n",
            "Epoch 56/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2741\n",
            "Epoch 57/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2725\n",
            "Epoch 58/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2698\n",
            "Epoch 59/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2685\n",
            "Epoch 60/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2669\n",
            "Epoch 61/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2651\n",
            "Epoch 62/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2636\n",
            "Epoch 63/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2622\n",
            "Epoch 64/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2611\n",
            "Epoch 65/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2587\n",
            "Epoch 66/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2575\n",
            "Epoch 67/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2566\n",
            "Epoch 68/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2555\n",
            "Epoch 69/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2540\n",
            "Epoch 70/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2533\n",
            "Epoch 71/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2527\n",
            "Epoch 72/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2508\n",
            "Epoch 73/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2498\n",
            "Epoch 74/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2483\n",
            "Epoch 75/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2476\n",
            "Epoch 76/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2456\n",
            "Epoch 77/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 1.2449\n",
            "Epoch 78/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2449\n",
            "Epoch 79/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2439\n",
            "Epoch 80/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2419\n",
            "Epoch 81/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 1.2425\n",
            "Epoch 82/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2424\n",
            "Epoch 83/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2390\n",
            "Epoch 84/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2398\n",
            "Epoch 85/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 1.2382\n",
            "Epoch 86/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2379\n",
            "Epoch 87/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2376\n",
            "Epoch 88/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2368\n",
            "Epoch 89/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2349\n",
            "Epoch 90/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2357\n",
            "Epoch 91/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2345\n",
            "Epoch 92/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2337\n",
            "Epoch 93/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2324\n",
            "Epoch 94/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2360\n",
            "Epoch 95/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2322\n",
            "Epoch 96/100\n",
            "2905/2905 [==============================] - 14s 5ms/step - loss: 1.2301\n",
            "Epoch 97/100\n",
            "2905/2905 [==============================] - 16s 6ms/step - loss: 1.2305\n",
            "Epoch 98/100\n",
            "2905/2905 [==============================] - 16s 5ms/step - loss: 1.2306\n",
            "Epoch 99/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2299\n",
            "Epoch 100/100\n",
            "2905/2905 [==============================] - 15s 5ms/step - loss: 1.2297\n",
            "ed had Baptista's youngest daughter.\n",
            "But, nobles; you have a title to be thing\n",
            "I sail, bring her mockers, to the beat!\n",
            "\n",
            "PROSCERO:\n",
            "Rum, forewer no married,\n",
            "Thou camples him feep appited days, have,\n",
            "Ahook the rest now but this brother foins,\n",
            "You have afred in this assain, indeed are\n",
            "The urtis against them partion of thy daughter\n",
            "\n",
            "JULIET:\n",
            "So you have with her sistam abries him often\n",
            "Whose trian'st news; and the we'll be more woe tell.\n",
            "\n",
            "TRA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "model.save('shakespeare_text_gen_model.h5')"
      ],
      "metadata": {
        "id": "t2iqabPTmGK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bdf619-734d-4c78-a59d-852e256b69d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIhuPMCsBInx"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}